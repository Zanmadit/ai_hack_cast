{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b6b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"asda.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc45ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        full_text += text + \"\\n\"\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # Clean the text\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text)\n",
    "    full_text = re.sub(r'\\n+', '\\n', full_text)\n",
    "    \n",
    "    return full_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcc1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='colbert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbc0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512, overlap=128):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks\n",
    "    chunk_size: approximate number of words per chunk\n",
    "    overlap: number of overlapping words between chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41b10468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert import Indexer\n",
    "from colbert.infra import Run, RunConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "def initialize_colbert(index_name=\"pdf_index\", experiment_name=\"pdf_experiment\"):\n",
    "    \"\"\"Initialize ColBERT with configuration\"\"\"\n",
    "    \n",
    "    # Configure ColBERT run\n",
    "    with Run().context(RunConfig(nranks=1, experiment=experiment_name)):\n",
    "        config = {\n",
    "            'doc_maxlen': 512,  # Max document length in tokens\n",
    "            'query_maxlen': 128,  # Max query length in tokens\n",
    "            'dim': 256,  # Embedding dimension\n",
    "            'similarity': 'cosine',\n",
    "            'checkpoint': 'colbert-ir/colbertv2.0'  # Use ColBERT v2.0 checkpoint\n",
    "        }\n",
    "        \n",
    "    return config, index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "552661a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_collection(pdf_path):\n",
    "    \"\"\"Prepare document collection from PDF\"\"\"\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the text\n",
    "    chunks = chunk_text(text, chunk_size=512, overlap=128)\n",
    "    \n",
    "    # Create a collection with metadata\n",
    "    collection = []\n",
    "    metadata = []\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        collection.append(chunk)\n",
    "        metadata.append({\n",
    "            'chunk_id': idx,\n",
    "            'source': pdf_path,\n",
    "            'text': chunk[:100] + '...'  # Store preview\n",
    "        })\n",
    "    \n",
    "    return collection, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d9e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert import Indexer\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "import torch\n",
    "\n",
    "def index_documents(collection, index_name=\"pdf_index\"):\n",
    "    \"\"\"Index the document collection using ColBERT\"\"\"\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    with Run().context(RunConfig(nranks=1, gpus=1)):\n",
    "        config = ColBERTConfig(\n",
    "            doc_maxlen=512,\n",
    "            nbits=2,  # Compression bits\n",
    "            kmeans_niters=4,\n",
    "            checkpoint='colbert-ir/colbertv2.0'\n",
    "        )\n",
    "        \n",
    "        indexer = Indexer(checkpoint=\"colbert-ir/colbertv2.0\", config=config)\n",
    "        \n",
    "        # Create index\n",
    "        indexer.index(\n",
    "            name=index_name,\n",
    "            collection=collection,\n",
    "            overwrite=True\n",
    "        )\n",
    "        \n",
    "        indexer.get_index()\n",
    "    \n",
    "    return indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd057ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert import Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "import torch\n",
    "\n",
    "def search_pdf(query, index_name=\"pdf_index\", k=5):\n",
    "    \"\"\"Search the indexed PDF content\"\"\"\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device for search: {device}\")\n",
    "    \n",
    "    with Run().context(RunConfig(nranks=1, gpus=1)):\n",
    "        config = ColBERTConfig(\n",
    "            checkpoint='colbert-ir/colbertv2.0'\n",
    "        )\n",
    "        \n",
    "        searcher = Searcher(index=index_name, config=config)\n",
    "        \n",
    "        # Search - returns a Ranking object with passage_ids and scores\n",
    "        results = searcher.search(query, k=k)\n",
    "        \n",
    "        # Format results\n",
    "        search_results = []\n",
    "        \n",
    "        # ColBERT returns a list where each item is (passage_id, rank, score)\n",
    "        # or just the ranking indices\n",
    "        for rank, (passage_id, score) in enumerate(zip(results[0], results[1])):\n",
    "            search_results.append({\n",
    "                'passage_id': passage_id,\n",
    "                'rank': rank + 1,\n",
    "                'score': score\n",
    "            })\n",
    "        \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b46aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_pdf_with_colbert(pdf_path, index_name=\"pdf_index\"):\n",
    "    \"\"\"Complete pipeline to embed PDF with ColBERT v2.0\"\"\"\n",
    "    \n",
    "    print(\"Step 1: Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Step 2: Chunking document...\")\n",
    "    chunks = chunk_text(text, chunk_size=256, overlap=32)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    print(\"Step 3: Initializing ColBERT...\")\n",
    "    config, index_name = initialize_colbert(index_name=index_name)\n",
    "    \n",
    "    print(\"Step 4: Preparing collection...\")\n",
    "    collection, metadata = prepare_collection(pdf_path)\n",
    "    \n",
    "    print(\"Step 5: Indexing documents...\")\n",
    "    indexer = index_documents(collection, index_name)\n",
    "    \n",
    "    print(\"Step 6: Index created successfully!\")\n",
    "    \n",
    "    # Save metadata for later reference\n",
    "    import json\n",
    "    with open(f\"{index_name}_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    return index_name, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc57a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Extracting text from PDF...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'asda.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_276132/3910531348.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __name__ == \u001b[33m\"__main__\"\u001b[39m:\n\u001b[32m      2\u001b[39m     pdf_file = \u001b[33m\"asda.pdf\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     index_name, metadata = embed_pdf_with_colbert(pdf_file)\n",
      "\u001b[32m/tmp/ipykernel_276132/1667813298.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(pdf_path, index_name)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m embed_pdf_with_colbert(pdf_path, index_name=\u001b[33m\"pdf_index\"\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[33m\"\"\"Complete pipeline to embed PDF with ColBERT v2.0\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m     print(\u001b[33m\"Step 1: Extracting text from PDF...\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     text = extract_text_from_pdf(pdf_path)\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m     print(\u001b[33m\"Step 2: Chunking document...\"\u001b[39m)\n\u001b[32m      8\u001b[39m     chunks = chunk_text(text, chunk_size=\u001b[32m256\u001b[39m, overlap=\u001b[32m32\u001b[39m)\n",
      "\u001b[32m/tmp/ipykernel_276132/790727864.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m extract_text_from_pdf(pdf_path):\n\u001b[32m      5\u001b[39m     \u001b[33m\"\"\"Extract text from PDF file\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     doc = fitz.open(pdf_path)\n\u001b[32m      7\u001b[39m     full_text = \u001b[33m\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;28;01min\u001b[39;00m range(doc.page_count):\n",
      "\u001b[32m~/ai_hack_cast/venv/lib/python3.13/site-packages/pymupdf/__init__.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[39m\n\u001b[32m   3030\u001b[39m                     self.page_count2 = extra.page_count_pdf\n\u001b[32m   3031\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3032\u001b[39m                     self.page_count2 = extra.page_count_fz\n\u001b[32m   3033\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3034\u001b[39m             JM_mupdf_show_errors = JM_mupdf_show_errors_old\n",
      "\u001b[31mFileNotFoundError\u001b[39m: no such file: 'asda.pdf'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_file = \"../asda.pdf\"\n",
    "    index_name, metadata = embed_pdf_with_colbert(pdf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
